[gh-actions]
python =
{%- if cookiecutter.jupyter_notebook_project == 'yes' %}
    3.7: nbqaxdoctest
    3.8: nbqaxdoctest
    3.9: nbqaxdoctest
{%- elif cookiecutter.jupyter_notebook_project == 'hybrid' %}
    3.7: py37, nbqaxdoctest
    3.8: py38, nbqaxdoctest
    3.9: py39, coverage, nbqaxdoctest
{%- else %}
    3.7: py37
    3.8: py38
    3.9: py39, coverage
{%- endif %}

[tox]
skip_missing_interpreters = true
isolated_build = true
envlist =
{%- if cookiecutter.jupyter_notebook_project != 'yes' %}
    py3{7,8,9},
    coverage,
{%- endif %}
{%- if cookiecutter.jupyter_notebook_project != 'no' %}
    nbqaxdoctest,
{%- endif %}
{%- if cookiecutter.project_type == 'package' %}
    package
{%- endif %}

{%- if cookiecutter.jupyter_notebook_project != 'yes' %}
[testenv]
description = Run test suite under {basepython}
setenv =
    PIP_DISABLE_VERSION_CHECK = 1
    COVERAGE_FILE = {env:COVERAGE_FILE:{toxworkdir}/.coverage.{envname}}
    PY_COLORS = 1
deps = -r requirements-dev.txt
passenv =
    http_proxy
    https_proxy
    no_proxy
commands = coverage run -m pytest \
           --benchmark-disable \
           --cov={toxinidir}/{{cookiecutter.package_name}} \
           --cov-config={toxinidir}/pyproject.toml \
           --junitxml={toxworkdir}/junit.{envname}.xml \
           --basetemp={envtmpdir} \
           -n={env:PYTEST_XDIST_PROC_NR:auto} \
           {posargs:tests}
wheel = true
wheel_pep517 = true
parallel_show_output = True

[testenv:py3{7,8,9}-benchmark]
description = Run performance testing under {basepython}
setenv = {[testenv]setenv}
deps = {[testenv]deps}
passenv = {[testenv]passenv}
allowlist_externals = bash
commands =
# Run benchmark in a shell to incorporate dynamic tag in output file names
    bash -c '\
        BENCHMARK_FILE_TAG=$(make strong-version-tag); \
        BENCHMARK_FILE_PREFIX="{toxinidir}/.benchmarks/{envname}/$\{BENCHMARK_FILE_TAG\}"; \
        mkdir -p $\{BENCHMARK_FILE_PREFIX%/*\}; \
        pytest \
            --basetemp={envtmpdir} \
            --benchmark-cprofile=cumtime \
            --benchmark-disable-gc \
            --benchmark-histogram="$\{BENCHMARK_FILE_PREFIX\}" \
            --benchmark-json="$\{BENCHMARK_FILE_PREFIX\}".json \
            --benchmark-only \
            --benchmark-save="$\{BENCHMARK_FILE_TAG\}" \
            --benchmark-save-data \
            --benchmark-sort=mean \
            --benchmark-verbose \
            {posargs:tests}'
wheel = true
wheel_pep517 = true
parallel_show_output = {[testenv]parallel_show_output}

[testenv:mutmut]
setenv =
    {[testenv]setenv}
    JUNIT_XML_REPORT_OUTPUT_ROOT={toxworkdir}/junit.{envname}
    JUNIT_XML_REPORT_OUTPUT_PATH={env:JUNIT_XML_REPORT_OUTPUT_ROOT}.xml
    JUNIT_HTML_REPORT_OUTPUT_PATH={env:JUNIT_XML_REPORT_OUTPUT_ROOT}.html
# To read emoji in the mutmut source:
    LANG=en_US.UTF-8
deps =
    {[testenv]deps}
    nodeenv
passenv = {[testenv]passenv}
allowlist_externals = bash
commands =
# Run mutmut in a shell to remove escape metacharacters in "{[testenv]commands}"
# Note: mutmut 1 exit code is a fatal error.
#   Other non-zero exit codes correspond to mutant statuses but indicate that
#   mutmut itself was successful and therefore are ignored for this test environment.
#   see: https://github.com/boxed/mutmut/blob/8bd1331429234ba572471d2ab6854b38e880f25d/mutmut/__init__.py#L1258
    bash -c '\
        mutmut run \
            --paths-to-mutate={{cookiecutter.package_name}}/ \
            --runner="{[testenv]commands}" \
            --tests-dir=tests/ || \
        MUTMUT_EXIT_CODE=$?; \
        if [[ "$\{MUTMUT_EXIT_CODE\}" -eq 1 ]]; \
            then \
                exit "$\{MUTMUT_EXIT_CODE\}"; \
        fi'
    mutmut results

# Export JUnit XML results to user-friendly static HTML page
    bash -c '\
        mutmut junitxml \
        > {env:JUNIT_XML_REPORT_OUTPUT_PATH}'

    nodeenv --python-virtualenv --jobs=8
    npm install -g --no-package-lock --no-save xunit-viewer
    # Inside of the virtualenv, '-g' installs into the virtualenv,
    # as opposed to installing into the local `node_modules`.

    xunit-viewer \
    --results {env:JUNIT_XML_REPORT_OUTPUT_PATH} \
    --title="Mutation Tests Report" \
    --output {env:JUNIT_HTML_REPORT_OUTPUT_PATH}
    bash -c '\
        echo -e "Test report available at \033[4;36m \
            {env:JUNIT_HTML_REPORT_OUTPUT_PATH} \
            \033[0m"'
wheel = true
wheel_pep517 = true

[testenv:coverage]
description = [Run locally after tests]: Combine coverage data and create reports;
              generates a diff coverage against `{env:DIFF_AGAINST:origin/master}`
              (can be changed by setting `DIFF_AGAINST` env var)
deps =
    coverage[toml]
    diff_cover
skip_install = true
passenv =
    {[testenv]passenv}
    DIFF_AGAINST
setenv = COVERAGE_FILE={toxworkdir}/.coverage
commands =
    coverage combine {toxworkdir}
    coverage report
    coverage xml -o {toxworkdir}/coverage.xml
    coverage html -d {toxworkdir}/htmlcov
    coverage json -o {toxworkdir}/coverage.json --pretty-print
    diff-cover {toxworkdir}/coverage.xml \
        --compare-branch={env:DIFF_AGAINST:origin/master} \
        --diff-range-notation '..'
depends =
    py3{7,8,9}
parallel_show_output = {[testenv]parallel_show_output}
{%- endif %}

{% if cookiecutter.jupyter_notebook_project != 'no' %}
[testenv:nbqaxdoctest]
description = Run notebook doctests directly via `xdoctest` (more efficient and
              less error-prone than using the built-in `doctest` library or
              running `xdoctest` via the `pytest` plugin). Note: if any test
              fails, the test environment command fails with the FIRST non-zero
              exit code encountered in the output job log.
deps = -r requirements-dev.txt
allowlist_externals = bash
commands =
    bash -c '\
        find {toxinidir}/{{cookiecutter.package_name}} \
            -type d -name .ipynb_checkpoints -prune -false -o \
            -type f -name "*.ipynb" \
        | parallel --joblog {toxworkdir}/{envname}.log -j 0 -k nbqa xdoctest \{\} --colored true; \
        NUM_FAILED_JOBS=$?; \
        echo "*****************$NUM_FAILED_JOBS FAILED JOBS*****************"; \
        cat {toxworkdir}/{envname}.log| awk "\$7 != 0"; \
        FAILURE_EXIT_CODES=$(cat {toxworkdir}/{envname}.log | awk "NR>1 && \$7 != 0 \{print \$7 \}") && \
        exit $\{FAILURE_EXIT_CODES:0:1\}'
# Run after testenv since auto-generated temp `.py` files interfere with pytest
depends =
    py3{7,8,9}
wheel = true
wheel_pep517 = true
parallel_show_output = {[testenv]parallel_show_output}
{%- endif %}

[testenv:precommit]
description = Run `pre-commit` hooks to auto-format and lint the codebase
deps =
{%- if cookiecutter.jupyter_notebook_project == 'yes' %}
    bandit # `nbqa-bandit`
{%- endif %}
{%- if cookiecutter.project_boilerplate_type != 'cli' %}
    importlib-metadata
{%- endif %}
    pre-commit
    # Pre-commit hooks that rely on finding their executables in the `.tox/precommit/bin`
    mypy
{%- if cookiecutter.jupyter_notebook_project == 'yes' %}
    nbqa
{%- endif %}
    pylint
    # Dependencies that are imported in source code and therefore needed for above pre-commit hook execution
    emoji
    hypothesis
    icontract
    icontract_hypothesis
    pytest
    pytest-benchmark
    sphinx
    typeguard
    # PEP 561 compliant stub packages for mypy
    types-emoji
    # Prod dependencies
    -r requirements.txt
skip_install = true
; For shfmt
setenv = GOCACHE={envdir}/go-build
passenv =
    HOMEPATH  # needed on Windows
    PROGRAMDATA  # needed on Windows
    SKIP # hook ids to skip
    TERM # (e.g. for mypy color output (https://github.com/tox-dev/tox/issues/1441#issuecomment-548063521))
commands = pre-commit run {posargs} -vv --all-files --color always
           python -c 'print("hint: run `make install-pre-commit-hooks` to add checks as pre-commit hook")'

{%- if cookiecutter.project_type == 'package' %}
[testenv:package]
description = Build Python package and validate its long description
deps =
    poetry >= 1.1.2
    mypy >= 0.910
    twine >= 1.12.1
    readme-renderer[md] >= 24.0
skip_install = true
commands = poetry build
           twine check {toxinidir}/dist/*
{%- endif %}

[testenv:security]
skip_install = true
deps = safety
commands = safety check --full-report -r {toxinidir}/requirements-all.txt

[testenv:docs]
extras =
    docs
passenv = TERM
setenv =
    PYTHONHASHSEED = 0
commands =
    sphinx-build -n -T -W -b html -d {envtmpdir}/doctrees docs/source docs/_build/html
    sphinx-build -n -T -W -b doctest -d {envtmpdir}/doctrees docs/source docs/_build/html
